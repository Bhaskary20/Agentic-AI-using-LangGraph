{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "2fae7d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "800711fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2859fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    task=\"text-generation\")\n",
    "model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "be022bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentSchema(BaseModel):\n",
    "\n",
    "    sentiment : Literal['positive', 'negative'] = Field(description=\"The sentiment of the review\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f3743cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagnosisSchema(BaseModel):\n",
    "\n",
    "    issue_type: Literal['billing', 'technical', 'account', 'other'] = Field(description=\"The type of issue the customer is facing\")\n",
    "    tone: Literal['angry', 'sad', 'neutral', 'happy'] = Field(description=\"The tone of the customer\")\n",
    "    urgency: Literal['low', 'medium', 'high'] = Field(description=\"The urgency of the issue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ad2be07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser2=PydanticOutputParser(pydantic_object=DiagnosisSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c392679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser= PydanticOutputParser(pydantic_object=SentimentSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "eaca045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2= PromptTemplate(\n",
    "    template=(\n",
    "        \"Analyze the following customer review and provide a diagnosis in JSON format.\\n\\n\"\n",
    "        \"Review: {review}\\n\\n\"\n",
    "        \"Return your answer in the following JSON format:\\n{response_format}\"\n",
    "    ),\n",
    "    input_variables=['review'],\n",
    "    partial_variables={'response_format': parser2.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "0ca34794",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_model2= prompt2 | model | parser2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "45d5b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"Classify the sentiment of this review as positive or negative.\\n\\n\"\n",
    "        \"Review: {review}\\n\\n\"\n",
    "        \"Return your answer in the following JSON format:\\n{response_format}\"\n",
    "    ),\n",
    "    input_variables=['review'],\n",
    "    partial_variables={'response_format': parser.get_format_instructions()}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "542f508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_model= prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "b18c7abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewState(TypedDict):\n",
    "\n",
    "    review: str\n",
    "    sentiment: Literal['positive', 'negative']\n",
    "    diagnosis: dict\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "9821681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentiment(state: ReviewState):\n",
    "    review = state['review']\n",
    "    prompt = f\"find the sentiment of this review \\n {state['review']}\"\n",
    "    sentiment = structured_model.invoke(prompt).sentiment\n",
    "    return {'sentiment': sentiment}   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2895de6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_condition(state: ReviewState) -> Literal['positive_response', 'run_diagnosis']:\n",
    "\n",
    "    if state['sentiment']=='positive':\n",
    "        return 'positive_response'\n",
    "    else:\n",
    "        return 'run_diagnosis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "4401da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_response(state: ReviewState):\n",
    "\n",
    "    prompt=f\"write a positive response to this review: {state['review']}\"\n",
    "    response=model.invoke(prompt).content\n",
    "    return {'response': response}\n",
    "\n",
    "def run_diagnosis(state: ReviewState):\n",
    "\n",
    "    prompt=f\"\"\"Diagnose this negative review:\\n\\n{state['review']}\\n\"\n",
    "    \"Return issue_type, tone, and urgency.\n",
    "\"\"\"\n",
    "    response=structured_model2.invoke(prompt)\n",
    "    return {'diagnosis': response.model_dump()}\n",
    "\n",
    "def negative_response(state: ReviewState):\n",
    "\n",
    "    prompt=f\"write a sympathetic response to this review: {state['review']}. The issue type is {state['diagnosis']['issue_type']}, the tone is {state['diagnosis']['tone']}, and the urgency is {state['diagnosis']['urgency']}.\"\n",
    "    response=model.invoke(prompt).content\n",
    "    return {'response': response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "98cd05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph=StateGraph(ReviewState)\n",
    "\n",
    "graph.add_node('find_sentiment', find_sentiment)\n",
    "graph.add_node('run_diagnosis', run_diagnosis)\n",
    "graph.add_node('positive_response', positive_response)\n",
    "graph.add_node('negative_response', negative_response)\n",
    "\n",
    "graph.add_edge(START, 'find_sentiment')\n",
    "graph.add_conditional_edges('find_sentiment', check_condition)\n",
    "graph.add_edge('run_diagnosis', 'negative_response')\n",
    "graph.add_edge('negative_response', END)\n",
    "graph.add_edge('positive_response', END)\n",
    "\n",
    "\n",
    "workflow=graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "b00c4321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': 'this is the worst product I have ever bought. It broke after one use and the customer service was terrible. I want a refund!',\n",
       " 'sentiment': 'negative',\n",
       " 'diagnosis': {'issue_type': 'technical', 'tone': 'angry', 'urgency': 'high'},\n",
       " 'response': \"I'm really sorry to hear that you had such a disappointing experience with our product. I understand your frustration, and I apologize for the inconvenience this has caused you. Let me see what I can do to help.\\n\\nFirst, I'd like to look into the technical issue you mentioned. Can you please describe in more detail what happened when the product broke after one use? This will help me understand the problem and provide a suitable solution.\\n\\nAs for the customer service, I would love to hear more about your interaction with us so I can work on improving it in the future.\\n\\nIt's not our intention to let our customers down, and I am committed to making this right for you. A refund is definitely an option if we can't resolve the technical issue to your satisfaction.\\n\\nThank you for your patience and understanding. Please let me know more about the issues you experienced so I can help.\\n\"}"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state={\n",
    "    'review': 'this is the worst product I have ever bought. It broke after one use and the customer service was terrible. I want a refund!'}\n",
    "\n",
    "workflow.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ea3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
